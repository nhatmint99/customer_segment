import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import linkage, fcluster
# from pyspark.sql import SparkSession
# from pyspark.ml.feature import VectorAssembler, StandardScaler as SparkScaler
# from pyspark.ml.clustering import KMeans as SparkKMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score

# ===============================
# Load Data
# ===============================
@st.cache_data
def load_data():
    df_products = pd.read_csv("Products_with_Categories.csv")
    df_trans = pd.read_csv("Transactions.csv")
    return df_products, df_trans

df_products, df_trans = load_data()

def customer_features(products, transactions):
    df = transactions.merge(products, on="productId", how="left")
    # RFM calculation
    df['TransactionDate'] = pd.to_datetime(df['Date'])
    max_date = df['TransactionDate'].max()
    df['Amount'] = df['items'] * df['price']
    customer_features = df.groupby("Member_number").agg({
        "TransactionDate": lambda x: (max_date - x.max()).days,  # Recency
        "productId": "count",                                    # Frequency
        "Amount": "sum",                                         # Monetary
        "Category": pd.Series.nunique                            # Category Diversity
    }).reset_index()

    customer_features.columns = ["Member_number", "Recency", "Frequency", "Monetary", "Category_Diversity"]
        # âœ… Fill NaN trÆ°á»›c khi scale
    customer_features = customer_features.fillna({
        "Recency": customer_features["Recency"].max(),  # giáº£ sá»­ khÃ¡ch chÆ°a mua â†’ recency lá»›n nháº¥t
        "Frequency": 0,
        "Monetary": 0,
        "Category_Diversity": 0
    })
    # Chuáº©n hÃ³a dá»¯ liá»‡u
    scaler = StandardScaler()
    X = scaler.fit_transform(customer_features.drop("Member_number", axis=1))

    # --- Clustering ---
    # KMeans
    kmeans_model = KMeans(n_clusters=4, random_state=42, n_init=10)
    customer_features['cluster_kmeans'] = kmeans_model.fit_predict(X)

    # GMM
    gmm_model = GaussianMixture(n_components=4, random_state=42)
    customer_features['cluster_gmm'] = gmm_model.fit_predict(X)

    # Agglomerative
    agg_model = AgglomerativeClustering(n_clusters=4)
    customer_features['cluster_agg'] = agg_model.fit_predict(X)

    # Hierarchical (Scipy)
    Z = linkage(X, method='ward')
    customer_features['cluster_hier'] = fcluster(Z, 4, criterion='maxclust')

    # Manual RFM segmentation
    #  --- Manual RFM segmentation ---
    try:
        r_labels = pd.qcut(customer_features["Recency"], 4, labels=[4,3,2,1], duplicates='drop')
        f_labels = pd.qcut(customer_features["Frequency"], 4, labels=[1,2,3,4], duplicates='drop')
        m_labels = pd.qcut(customer_features["Monetary"], 4, labels=[1,2,3,4], duplicates='drop')

        customer_features["R_quartile"] = r_labels.cat.codes.replace(-1, 0)  # -1 -> 0
        customer_features["F_quartile"] = f_labels.cat.codes.replace(-1, 0)
        customer_features["M_quartile"] = m_labels.cat.codes.replace(-1, 0)

        customer_features["RFM_Score"] = (
            customer_features["R_quartile"] + 
            customer_features["F_quartile"] + 
            customer_features["M_quartile"]
        )
        customer_features["cluster_rfm"] = pd.qcut(customer_features["RFM_Score"], 4, labels=[0,1,2,3], duplicates='drop')
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi tÃ­nh RFM quartiles: {e}")
        customer_features["R_quartile"] = 0
        customer_features["F_quartile"] = 0
        customer_features["M_quartile"] = 0
        customer_features["RFM_Score"] = 0
        customer_features["cluster_rfm"] = 0
    # Spark ML
    # spark = SparkSession.builder.appName("CustomerSegmentation").getOrCreate()
    # spark_df = spark.createDataFrame(customer_features.drop(columns=[
    #     "cluster_kmeans","cluster_gmm","cluster_agg","cluster_hier","cluster_rfm",
    #     "R_quartile","F_quartile","M_quartile","RFM_Score"
    # ]))

    # assembler = VectorAssembler(inputCols=["Recency","Frequency","Monetary","Category_Diversity"], outputCol="features")
    # assembled = assembler.transform(spark_df).cache()

    # scaler_spark = SparkScaler(inputCol="features", outputCol="scaled_features", withStd=True, withMean=True)
    # scaler_model_spark = scaler_spark.fit(assembled)
    # scaled = scaler_model_spark.transform(assembled).cache()

    # spark_kmeans = SparkKMeans(k=4, seed=42, featuresCol="scaled_features", predictionCol="cluster_spark")
    # spark_kmeans_model = spark_kmeans.fit(scaled)
    # spark_clustered = spark_kmeans_model.transform(scaled)

    # df_spark_result = spark_clustered.select("Member_number", "cluster_spark").toPandas()
    # customer_features = customer_features.merge(df_spark_result, on="Member_number")

    return customer_features, X
# ===============================
# Compute RFM scores automatically
# ===============================
def compute_rfm_scores(df, user_id_col="Member_number", date_col="Date", prod_col="productId", amount_col=None):
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    snapshot_date = df[date_col].max() + pd.Timedelta(days=1)

    # Frequency = number of purchases
    freq = df.groupby(user_id_col)[prod_col].count()

    # Monetary
    if amount_col and amount_col in df.columns:
        monetary = df.groupby(user_id_col)[amount_col].sum()
    else:
        monetary = freq.copy()

    # Recency
    rec = df.groupby(user_id_col)[date_col].max().apply(lambda x: (snapshot_date - x).days)

    rfm = pd.DataFrame({"Recency": rec, "Frequency": freq, "Monetary": monetary})

    # âœ… Fix NaN trÆ°á»›c khi phÃ¢n loáº¡i
    rfm = rfm.fillna({"Recency": rfm["Recency"].max(), "Frequency": 0, "Monetary": 0})

    # âœ… DÃ¹ng cat.codes thay vÃ¬ astype(int)
    rfm["R"] = pd.qcut(rfm["Recency"], 4, labels=[4,3,2,1], duplicates="drop").cat.codes.replace(-1, 0)
    rfm["F"] = pd.qcut(rfm["Frequency"], 4, labels=[1,2,3,4], duplicates="drop").cat.codes.replace(-1, 0)
    rfm["M"] = pd.qcut(rfm["Monetary"], 4, labels=[1,2,3,4], duplicates="drop").cat.codes.replace(-1, 0)

    return rfm.reset_index()

try:
    rfm_scores = compute_rfm_scores(df_trans)
except Exception as e:
    st.warning(f"Could not compute RFM automatically: {e}")
    rfm_scores = None

# ===============================
# Classification into 6 segments
# ===============================
def classify_customer(R, F, M):
    if R >= 3 and F >= 3 and M == 4:
        return """VIP - KhÃ¡ch hÃ ng trung thÃ nh, giÃ¡ trá»‹ cao 
    -> ThÆ°á»Ÿng báº±ng chÆ°Æ¡ng trÃ¬nh VIP, Æ°u Ä‘Ã£i Ä‘á»™c quyá»n 
    & Cho quyá»n truy cáº­p sá»›m sáº£n pháº©m/dá»‹ch vá»¥ má»›i 
    & Khuyáº¿n khÃ­ch há» trá»Ÿ thÃ nh Ä‘áº¡i sá»© thÆ°Æ¡ng hiá»‡u"""
    elif F >= 3 and R >= 2 and M >= 2:
        return """Loyal Customers (KhÃ¡ch hÃ ng trung thÃ nh tiá»m nÄƒng) 
    -> TÄƒng cÆ°á»ng gáº¯n káº¿t báº±ng Æ°u Ä‘Ã£i Ä‘á»‹nh ká»³ 
    & Cung cáº¥p gÃ³i combo/bundles phÃ¹ há»£p 
    & ChÄƒm sÃ³c cÃ¡ nhÃ¢n hÃ³a Ä‘á»ƒ Ä‘áº©y lÃªn nhÃ³m VIP"""
    elif R == 2 and F <= 1 and M <= 1:
        return """At Risk/ Lost (KhÃ¡ch hÃ ng cÃ³ nguy cÆ¡ rá»i bá») 
    -> Giáº£m giÃ¡ sáº£n pháº©m yÃªu thÃ­ch trÆ°á»›c Ä‘Ã¢y 
    & TÃ¬m hiá»ƒu lÃ½ do há» Ã­t quay láº¡i (kháº£o sÃ¡t) 
    & Cung cáº¥p Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t Ä‘á»ƒ khuyáº¿n khÃ­ch há» quay láº¡i 
    & Táº¡o cáº£m giÃ¡c cáº¥p bÃ¡ch vá»›i Æ°u Ä‘Ã£i giá»›i háº¡n thá»i gian"""
    else:
        return """New/ Regular Customers (KhÃ¡ch hÃ ng vÃ£ng lai) 
    -> Chiáº¿n dá»‹ch win-back vá»›i khuyáº¿n mÃ£i lá»›n 
    & Táº¡o ná»™i dung khuyáº¿n mÃ£i Ä‘á»ƒ thu hÃºt khÃ¡ch hÃ ng má»›i 
    & Xin Ä‘Ã¡nh giÃ¡ sáº£n pháº©m/dá»‹ch vá»¥ Ä‘á»ƒ cáº£i thiá»‡n """

# ===============================
# Classification from raw values (not quartiles)
# ===============================
def classify_customer_raw(recency, frequency, monetary,
                          r_thresh, f_thresh, m_thresh):
    """
    PhÃ¢n loáº¡i khÃ¡ch hÃ ng dá»±a trÃªn giÃ¡ trá»‹ R, F, M gá»‘c.
    - recency: sá»‘ ngÃ y ká»ƒ tá»« láº§n mua gáº§n nháº¥t
    - frequency: sá»‘ láº§n mua
    - monetary: tá»•ng chi tiÃªu - (Ä‘Æ¡n vá»‹: nghÃ¬n Ä‘á»“ng)
    NgÆ°á»¡ng cÃ³ thá»ƒ Ä‘iá»u chá»‰nh theo dá»¯ liá»‡u thá»±c táº¿.
    """
    if recency <= r_thresh/2 and frequency >= f_thresh and monetary >= m_thresh*1.5:
        return """VIP - KhÃ¡ch hÃ ng trung thÃ nh, giÃ¡ trá»‹ cao 
    -> ThÆ°á»Ÿng báº±ng chÆ°Æ¡ng trÃ¬nh VIP, Æ°u Ä‘Ã£i Ä‘á»™c quyá»n 
    & Cho quyá»n truy cáº­p sá»›m sáº£n pháº©m/dá»‹ch vá»¥ má»›i 
    & Khuyáº¿n khÃ­ch há» trá»Ÿ thÃ nh Ä‘áº¡i sá»© thÆ°Æ¡ng hiá»‡u"""
    elif recency <= r_thresh and frequency >= f_thresh/2:
        return """Loyal Customers (KhÃ¡ch hÃ ng trung thÃ nh tiá»m nÄƒng) 
    -> TÄƒng cÆ°á»ng gáº¯n káº¿t báº±ng Æ°u Ä‘Ã£i Ä‘á»‹nh ká»³ 
    & Cung cáº¥p gÃ³i combo/bundles phÃ¹ há»£p 
    & ChÄƒm sÃ³c cÃ¡ nhÃ¢n hÃ³a Ä‘á»ƒ Ä‘áº©y lÃªn nhÃ³m VIP"""
    elif recency > r_thresh*3 and frequency <= f_thresh/2 and monetary <= m_thresh/2:
        return """At Risk/ Lost (KhÃ¡ch hÃ ng cÃ³ nguy cÆ¡ rá»i bá») 
    -> Giáº£m giÃ¡ sáº£n pháº©m yÃªu thÃ­ch trÆ°á»›c Ä‘Ã¢y 
    & TÃ¬m hiá»ƒu lÃ½ do há» Ã­t quay láº¡i (kháº£o sÃ¡t) 
    & Cung cáº¥p Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t Ä‘á»ƒ khuyáº¿n khÃ­ch há» quay láº¡i 
    & Táº¡o cáº£m giÃ¡c cáº¥p bÃ¡ch vá»›i Æ°u Ä‘Ã£i giá»›i háº¡n thá»i gian"""
    else:
        return """New/ Regular Customers (KhÃ¡ch hÃ ng vÃ£ng lai) 
    -> Chiáº¿n dá»‹ch win-back vá»›i khuyáº¿n mÃ£i lá»›n 
    & Táº¡o ná»™i dung khuyáº¿n mÃ£i Ä‘á»ƒ thu hÃºt khÃ¡ch hÃ ng má»›i 
    & Xin Ä‘Ã¡nh giÃ¡ sáº£n pháº©m/dá»‹ch vá»¥ Ä‘á»ƒ cáº£i thiá»‡n """

# Optimized clustering evaluation and comparison

def evaluate_clustering_performance(scaled_features, customer_data):
    """
    Evaluate clustering methods with Silhouette, ARI, NMI
    """
    clustering_methods = {
        "KMeans": "cluster_kmeans",
        "GMM": "cluster_gmm",
        "Agglomerative": "cluster_agg", 
        "Hierarchical": "cluster_hier",
        "SparkKMeans": "cluster_spark"
    }
    
    # --- Silhouette Scores ---
    silhouette_scores = {}
    for method_name, cluster_col in clustering_methods.items():
        if cluster_col in customer_data.columns:
            score = silhouette_score(scaled_features, customer_data[cluster_col])
            silhouette_scores[method_name] = round(score, 4)
    
    # --- Pairwise ARI & NMI ---
    similarity_results = []
    method_columns = list(clustering_methods.values())

    for i in range(len(method_columns)):
        for j in range(i+1, len(method_columns)):
            col1, col2 = method_columns[i], method_columns[j]
            if col1 in customer_data.columns and col2 in customer_data.columns:
                ari = adjusted_rand_score(customer_data[col1], customer_data[col2])
                nmi = normalized_mutual_info_score(customer_data[col1], customer_data[col2])
                similarity_results.append([
                    col1.replace('cluster_', '').title(),
                    col2.replace('cluster_', '').title(),
                    round(ari, 4),
                    round(nmi, 4)
                ])
    
    similarity_df = pd.DataFrame(similarity_results, columns=["Method A", "Method B", "ARI", "NMI"])
    return silhouette_scores, similarity_df


# ===============================
# Sidebar Menu
# ===============================
menu = st.sidebar.radio(
    "Menu",
    [
        "Introduction",
        "Business Problem",
        "Evaluation & Report & Comparison",
        "New Prediction / Analysis"
    ]
)
st.sidebar.header("Data")
product_up = st.sidebar.file_uploader("Upload Products_with_categories.csv", type=["csv"])
transaction_up = st.sidebar.file_uploader("Upload Transactions.csv", type=["csv"])
if product_up is None:
    df_products = pd.read_csv("Products_with_Categories.csv")
if transaction_up is None:
    df_trans = pd.read_csv("Transactions.csv")
st.sidebar.markdown("---")
# ===============================
# Sidebar Threshold Settings (for raw RFM)
# ===============================
st.sidebar.markdown("### âš™ï¸ Thiáº¿t láº­p ngÆ°á»¡ng RFM (Raw Values)")
r_thresh = st.sidebar.number_input("NgÆ°á»¡ng Recency (ngÃ y)", min_value=1, value=90)
f_thresh = st.sidebar.number_input("NgÆ°á»¡ng Frequency (sá»‘ láº§n mua)", min_value=1, value=10)
m_thresh = st.sidebar.number_input("NgÆ°á»¡ng Monetary (tá»•ng chi tiÃªu - Ä‘vt: nghÃ¬n Ä‘á»“ng)", min_value=1, value=2000)
st.sidebar.markdown("---")

# ===============================
# Business Problem
# ===============================
if menu == "Business Problem":
    st.title("ğŸ“Œ Business Problem")
    st.markdown("""
    Cá»­a hÃ ng X chá»§ yáº¿u bÃ¡n cÃ¡c sáº£n pháº©m thiáº¿t yáº¿u cho khÃ¡ch hÃ ng nhÆ°:
    rau, cá»§, quáº£, thá»‹t, cÃ¡, trá»©ng, sá»¯a, nÆ°á»›c giáº£i khÃ¡t...
    KhÃ¡ch hÃ ng cá»§a cá»­a hÃ ng thÆ°á»ng lÃ  khÃ¡ch hÃ ng mua láº».
    """)
    st.markdown("""-> Chá»§ cá»­a hÃ ng X mong muá»‘n cÃ³ thá»ƒ bÃ¡n Ä‘Æ°á»£c nhiá»u hÃ ng hÃ³a hÆ¡n
    cÅ©ng nhÆ° giá»›i thiá»‡u sáº£n pháº©m Ä‘áº¿n Ä‘Ãºng Ä‘á»‘i tÆ°á»£ng khÃ¡ch hÃ ng,
    tá»« Ä‘Ã³ tÄƒng doanh thu vÃ  lá»£i nhuáº­n.
    Äá»“ng thá»i, viá»‡c phÃ¢n khÃºc khÃ¡ch hÃ ng cÅ©ng giÃºp cá»­a hÃ ng cÃ³ chiáº¿n lÆ°á»£c
    chÄƒm sÃ³c vÃ  lÃ m hÃ i lÃ²ng khÃ¡ch hÃ ng.
    """)
    st.image('customersegment.png', caption='Customer Segmentation', width=800)

# ===============================
# Evaluation & Report
# ===============================
elif menu == "Evaluation & Report & Comparison":
    st.title("ğŸ“Š Evaluation & Report & Comparison")
    st.image('segment1.png', caption='Customer Segment Pros', width=600)

    tab = st.tabs(["RFM Analysis, Segmentation & Visualization",
                "Clustering Model Comparison & Evaluation"])
    # ---------------- Tab 1 ----------------
    with tab[0]:
        # Æ¯u tiÃªn dá»¯ liá»‡u upload
        if product_up is not None:
            df_products = pd.read_csv(product_up)
            st.success("âœ… Products data uploaded successfully.")
            st.dataframe(df_products.head())
        else:
            df_products = df_products
            st.info("âš ï¸ DÃ¹ng dá»¯ liá»‡u Products máº·c Ä‘á»‹nh.")

        if transaction_up is not None:
            df_trans = pd.read_csv(transaction_up)
            st.success("âœ… Transactions data uploaded successfully.")
            st.dataframe(df_trans.head())
        else:
            df_trans = df_trans
            st.info("âš ï¸ DÃ¹ng dá»¯ liá»‡u Transactions máº·c Ä‘á»‹nh.")

        # TÃ­nh toÃ¡n RFM
        if rfm_scores is not None:
            rfm_scores = compute_rfm_scores(df_trans)
            rfm_scores["Segment_Raw"] = rfm_scores.apply(
                lambda x: classify_customer_raw(x["Recency"], x["Frequency"], x["Monetary"],
                                                r_thresh, f_thresh, m_thresh), axis=1
            )
            st.success("âœ… RFM scores computed successfully.")
        # except Exception as e:
        #     st.error(f"âŒ Lá»—i khi tÃ­nh RFM: {e}")
        #     rfm_scores = None

        # Náº¿u cÃ³ RFM thÃ¬ váº½ biá»ƒu Ä‘á»“
        if rfm_scores is not None:
            st.subheader("ğŸ“ˆ RFM Score Distribution")
            fig, axes = plt.subplots(1, 3, figsize=(15, 4))
            sns.countplot(x="R", data=rfm_scores, ax=axes[0])
            axes[0].set_title("Recency Score")
            sns.countplot(x="F", data=rfm_scores, ax=axes[1])
            axes[1].set_title("Frequency Score")
            sns.countplot(x="M", data=rfm_scores, ax=axes[2])
            axes[2].set_title("Monetary Score")
            st.pyplot(fig)

            # PhÃ¢n loáº¡i Segment
            rfm_scores["Segment"] = rfm_scores.apply(
                lambda x: classify_customer(x["R"], x["F"], x["M"]), axis=1
            )
            seg_counts = rfm_scores["Segment"].value_counts()

            st.bar_chart(seg_counts)
            st.dataframe(rfm_scores.head())
        else:
            st.warning("KhÃ´ng cÃ³ dá»¯ liá»‡u RFM Ä‘á»ƒ hiá»ƒn thá»‹.")

    # ---------------- Tab 2 ----------------
    with tab[1]:
        st.subheader("Clustering Model Comparison")

    try:
        customer_data, scaled_features = customer_features(df_products, df_trans)
        silhouette_scores, similarity_df = evaluate_clustering_performance(scaled_features, customer_data)

        # Hiá»ƒn thá»‹ Silhouette
        st.subheader("ğŸ“Š Silhouette Scores")
        df_metrics = pd.DataFrame(list(silhouette_scores.items()), columns=["Model", "Silhouette"])
        st.table(df_metrics)

        # Hiá»ƒn thá»‹ ARI & NMI
        st.subheader("ğŸ”— Similarity Analysis (ARI & NMI)")
        st.dataframe(similarity_df)

        # Heatmap
        fig, ax = plt.subplots(figsize=(6,4))
        pivot_table = similarity_df.pivot(index="Method A", columns="Method B", values="ARI")
        sns.heatmap(pivot_table, annot=True, cmap="Blues", ax=ax)
        ax.set_title("Heatmap of ARI")
        st.pyplot(fig)
        plt.close(fig)

    except Exception as e:
        st.error(f"âŒ Lá»—i khi tÃ­nh toÃ¡n clustering: {e}")
        st.markdown("""ğŸ”¹ Káº¿t quáº£ so sÃ¡nh nhanh
- KMeans & SparkKMeans:
Silhouette tá»‘t nháº¥t (â‰ˆ0.285).
ARI & NMI gáº§nÂ 1.0Â â†’
Hai phÆ°Æ¡ng phÃ¡p nÃ y gáº§n nhÆ° giá»‘ng há»‡t nhau.
SparkKMeans cÃ³ lá»£i tháº¿ vá»Â kháº£ nÄƒng má»Ÿ rá»™ngÂ (phÃ¹ há»£p khi dá»¯ liá»‡u lá»›n).
- Agglomerative & Hierarchical:
Káº¿t quáº£ giá»‘ng há»‡t nhau (ARI = 1.0, NMI = 1.0).
Tuy nhiÃªn, Silhouette tháº¥p hÆ¡n (â‰ˆ0.216).
Æ¯u Ä‘iá»ƒm:Â trá»±c quan hoÃ¡ báº±ng dendrogram, dá»… giáº£i thÃ­ch má»‘i quan há»‡ phÃ¢n cáº¥p.
- GMM (Gaussian Mixture):
Silhouette tháº¥p nháº¥t (â‰ˆ0.159).
ARI & NMI vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c Ä‘á»u tháº¥p â†’ táº¡o ra phÃ¢n cá»¥m ráº¥t khÃ¡c biá»‡t.
Chá»‰ phÃ¹ há»£p náº¿u muá»‘nÂ phÃ¢n cá»¥m má»m (soft clustering), tá»©c má»™t khÃ¡ch hÃ ng cÃ³ thá»ƒ thuá»™c nhiá»u cá»¥m.
-> ğŸ”¹ Khuyáº¿n nghá»‹ cuá»‘i cÃ¹ng
- Chá»n KMeans (hoáº·c SparkKMeans náº¿u dá»¯ liá»‡u lá»›n)Â lÃ m phÆ°Æ¡ng phÃ¡p chÃ­nh Ä‘á»ƒ phÃ¢n cá»¥m RFM.
LÃ½ do: Ä‘iá»ƒm Silhouette tá»‘t nháº¥t, tÃ­nh á»•n Ä‘á»‹nh cao, dá»… Ã¡p dá»¥ng cho business.
SparkKMeans Ä‘áº·c biá»‡t phÃ¹ há»£p náº¿u dá»¯ liá»‡u ngÃ y cÃ ng má»Ÿ rá»™ng (big data).
- DÃ¹ng Agglomerative/HierarchicalÂ nhÆ° má»™tÂ phÆ°Æ¡ng phÃ¡p bá»• trá»£Â Ä‘á»ƒ kiá»ƒm tra láº¡i káº¿t quáº£ vÃ  trá»±c quan hÃ³a má»‘i quan há»‡ phÃ¢n cá»¥m.
- GMMÂ cÃ³ thá»ƒ thá»­ nghiá»‡m náº¿u muá»‘n kiá»ƒm tra xem khÃ¡ch hÃ ng cÃ³ thá»ƒ thuá»™cÂ nhiá»u nhÃ³m Ä‘á»“ng thá»iÂ (phÃ¢n tÃ­ch nÃ¢ng cao).
- CÃ³ thá»ƒ dÃ¹ng RFM Manual cho kháº£ nÄƒng tÆ°Æ¡ng tÃ¡c cá»§a end-user dá»… hÆ¡n
""")

# ===============================
# New Prediction (manual input)
# ===============================
elif menu == "New Prediction / Analysis":
    st.title("ğŸ”® Customer Prediction (Manual Input)")
    st.markdown("""
    - **VIP (High Frequency, High Monetary, Low Recency)** 
    -> Shop very often, spend a lot, and bought recently.
    - **Loyal Customer (Medium Frequency & Monetary, Recent Buyers)** 
    -> Bought recently, moderate spending, could become loyal. Theyâ€™reÂ on the path to becoming VIP.
    - **At Risk/ Lost Customer (Low Frequency, Low Monetary, High Recency)** 
    -> Send reminders, discounts to reactivate.  
    - **Regular/ New Customer (High Recency, Low Frequency & Monetary)** 
    -> Just purchased or bought only once. Still deciding whether to stick with you.
    """)
    tab = st.tabs(["RFM Manual Prediction", "KMeans Prediction (from Clustering)"])
    with tab[0]:
        st.subheader("RFM Manual Prediction")
        option = st.radio("Chá»n kiá»ƒu nháº­p dá»¯ liá»‡u:", ["Nháº­p Ä‘iá»ƒm RFM (1â€“4)", "Nháº­p giÃ¡ trá»‹ gá»‘c R, F, M"])
        if option == "Nháº­p Ä‘iá»ƒm RFM (1â€“4)":
        # ---------------------------
        # Option 1: nháº­p Ä‘iá»ƒm RFM
        # ---------------------------
                st.subheader("ğŸ”® Dá»± Ä‘oÃ¡n theo Ä‘iá»ƒm RFM (1â€“4) - Single prediction")
                col1, col2, col3 = st.columns(3)
                with col1:
                    R = st.slider("Recency Score (1â€“4)", 1, 4, 2)
                with col2:
                    F = st.slider("Frequency Score (1â€“4)", 1, 4, 2)
                with col3:
                    M = st.slider("Monetary Score (1â€“4)", 1, 4, 2)

                if st.button("Predict by Score"):
                    segment = classify_customer(R, F, M)
                    st.success(f"ğŸ·ï¸ This customer belongs to: **{segment}**")

                st.markdown("---")
                st.subheader("ğŸ“‚ Bulk Prediction (File RFM Score)")
                file = st.file_uploader("Táº£i lÃªn file CSV/Excel vá»›i cá»™t `R`, `F`, `M`", type=["csv","xlsx"])
                if file:
                    if file.name.endswith(".csv"):
                        df_input = pd.read_csv(file)
                    else:
                        df_input = pd.read_excel(file)

                    if {"R","F","M"}.issubset(df_input.columns):
                        df_input["Segment"] = df_input.apply(lambda x: classify_customer(x["R"], x["F"], x["M"]), axis=1)
                        st.dataframe(df_input.head(20))
                        st.download_button(
                            "Táº£i xuá»‘ng dá»± Ä‘oÃ¡n (RFM Score)",
                            data=df_input.to_csv(index=False).encode("utf-8"),
                            file_name="rfm_predictions_score.csv",
                            mime="text/csv"
                        )
                    else:
                        st.error("File pháº£i chá»©a cÃ¡c cá»™t `R`, `F`, `M`.")

        # ---------------------------
        # Option 2: nháº­p giÃ¡ trá»‹ gá»‘c
        # ---------------------------
        else: 
            st.markdown("---")
            st.subheader("ğŸ”® Dá»± Ä‘oÃ¡n theo giÃ¡ trá»‹ gá»‘c")

            col1, col2, col3 = st.columns(3)
            with col1:
                recency_val = st.number_input("Recency (ngÃ y ká»ƒ tá»« láº§n mua gáº§n nháº¥t)", min_value=0, value=100)
            with col2:
                frequency_val = st.number_input("Frequency (sá»‘ láº§n mua)", min_value=0, value=5)
            with col3:
                monetary_val = st.number_input("Monetary (tá»•ng chi tiÃªu - Ä‘Æ¡n vá»‹: nghÃ¬n Ä‘á»“ng)", min_value=0, value=200)

            if st.button("Predict by Raw Value"):
                segment = classify_customer_raw(recency_val, frequency_val, monetary_val,
                                                        r_thresh=r_thresh, f_thresh=f_thresh, m_thresh=m_thresh)
                st.success(f"ğŸ·ï¸ This customer belongs to: **{segment}**")

            st.markdown("---")
            st.subheader("ğŸ“‚ Bulk Prediction (File Raw Values)")
            file = st.file_uploader("Táº£i lÃªn file CSV/Excel vá»›i cá»™t `R - Recency`, `F - Frequency`, `M - Monetary`", type=["csv","xlsx"])
            if file:
                if file.name.endswith(".csv"):
                    df_input = pd.read_csv(file)
                else:
                    df_input = pd.read_excel(file)

                if {"Recency","Frequency","Monetary"}.issubset(df_input.columns):
                    df_input["Segment"] = df_input.apply(
                        lambda x: classify_customer_raw(
                            x["Recency"], x["Frequency"], x["Monetary"],
                            r_thresh=r_thresh, f_thresh=f_thresh, m_thresh=m_thresh
                            ),
                        axis=1
                        )
                    st.dataframe(df_input.head(20))
                    st.download_button(
                        "Táº£i xuá»‘ng dá»± Ä‘oÃ¡n (Raw Values)",
                        data=df_input.to_csv(index=False).encode("utf-8"),
                        file_name="rfm_predictions_raw.csv",
                        mime="text/csv"
                    )
                else:
                    st.error("File pháº£i chá»©a cÃ¡c cá»™t `R`, `F`, `M`.")
        with tab[1]:
            st.title("ğŸ’¡ Recommendations by KMeans Clusters")
            try:
                # cháº¡y láº¡i phÃ¢n cá»¥m KMeans
                customer_data, scaled_features = customer_features(df_products, df_trans)

                # Äáº¿m sá»‘ khÃ¡ch hÃ ng theo cá»¥m
                cluster_counts = customer_data["cluster_kmeans"].value_counts().sort_index()

                st.subheader("ğŸ“Š PhÃ¢n bá»‘ khÃ¡ch hÃ ng theo cá»¥m (KMeans)")
                st.bar_chart(cluster_counts)

                # Hiá»ƒn thá»‹ báº£ng khÃ¡ch hÃ ng máº«u
                st.dataframe(customer_data[["Member_number","Recency","Frequency","Monetary","cluster_kmeans"]].head())

                # Gá»£i Ã½ chiáº¿n lÆ°á»£c theo tá»«ng cá»¥m
                st.subheader("ğŸ’¡ Gá»£i Ã½ chiáº¿n lÆ°á»£c kinh doanh")

                recommendations = {
                    0: "Cluster 0 â€“ CÃ³ thá»ƒ lÃ  khÃ¡ch má»›i: Khuyáº¿n mÃ£i chÃ o má»«ng, Æ°u Ä‘Ã£i láº§n mua Ä‘áº§u.",
                    1: "Cluster 1 â€“ KhÃ¡ch trung thÃ nh giÃ¡ trá»‹ cao: ChÆ°Æ¡ng trÃ¬nh VIP, chÄƒm sÃ³c cÃ¡ nhÃ¢n hÃ³a.",
                    2: "Cluster 2 â€“ KhÃ¡ch Ã­t tÆ°Æ¡ng tÃ¡c, chi tiÃªu tháº¥p: Gá»­i chiáº¿n dá»‹ch tÃ¡i kÃ­ch hoáº¡t, giáº£m giÃ¡.",
                    3: "Cluster 3 â€“ KhÃ¡ch chi tiÃªu nhiá»u nhÆ°ng Ã­t mua: Khuyáº¿n khÃ­ch mua thÆ°á»ng xuyÃªn, gá»£i Ã½ combo."
                }

                for cluster, note in recommendations.items():
                    st.markdown(f"**Cá»¥m {cluster}:** {note}")

            except Exception as e:
                st.error(f"âŒ Lá»—i khi táº¡o recommendation tá»« KMeans: {e}")
# ===============================
# Introduction
# ===============================
elif menu == "Introduction":
    st.title("ğŸ‘¨â€ğŸ’» Introduction")
    st.markdown("""
    - **TÃªn**: Tráº§n Nháº­t Minh   
    - **Email**: nhatminhtr233@gmail.com   
    - **GVHD**: Khuáº¥t Thuá»³ PhÆ°Æ¡ng
    - **Project**: Customer Segmentation
    """)
    st.image("RFM_clustering.png", caption="RFM Clustering")
    st.subheader("ğŸ”„ Project Pipeline")
    st.markdown("""
        - Business Problem", "XÃ¡c Ä‘á»‹nh má»¥c tiÃªu kinh doanh, vÃ­ dá»¥: tÄƒng doanh thu, chÄƒm sÃ³c khÃ¡ch hÃ ng.
        - Data Preparation", "Thu tháº­p & lÃ m sáº¡ch dá»¯ liá»‡u sáº£n pháº©m vÃ  giao dá»‹ch.
        - RFM Analysis", "TÃ­nh toÃ¡n Recency, Frequency, Monetary cho tá»«ng khÃ¡ch hÃ ng.
        - Clustering Models", "Thá»­ nhiá»u mÃ´ hÃ¬nh: KMeans, GMM, Agglomerative, Hierarchical, SparkKMeans.
        - Evaluation", "ÄÃ¡nh giÃ¡ báº±ng Silhouette, ARI, NMI Ä‘á»ƒ so sÃ¡nh mÃ´ hÃ¬nh.
        - Recommendation & Deployment", "ÄÆ°a ra gá»£i Ã½ kinh doanh cho tá»«ng nhÃ³m khÃ¡ch hÃ ng vÃ  triá»ƒn khai há»‡ thá»‘ng."""
    )
    st.image('pipeline.png', caption = 'Project Pipeline')
