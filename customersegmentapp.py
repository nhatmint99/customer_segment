import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import linkage, fcluster
# from pyspark.sql import SparkSession
# from pyspark.ml.feature import VectorAssembler, StandardScaler as SparkScaler
# from pyspark.ml.clustering import KMeans as SparkKMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score

# ===============================
# Load Data
# ===============================
@st.cache_data
def load_data():
    df_products = pd.read_csv("Products_with_Categories.csv")
    df_trans = pd.read_csv("Transactions.csv")
    return df_products, df_trans

df_products, df_trans = load_data()

def customer_features(products, transactions):
    df = transactions.merge(products, on="productId", how="left")
    # RFM calculation
    df['TransactionDate'] = pd.to_datetime(df['Date'])
    max_date = df['TransactionDate'].max()
    df['Amount'] = df['items'] * df['price']
    customer_features = df.groupby("Member_number").agg({
        "TransactionDate": lambda x: (max_date - x.max()).days,  # Recency
        "productId": "count",                                    # Frequency
        "Amount": "sum",                                         # Monetary
        "Category": pd.Series.nunique                            # Category Diversity
    }).reset_index()

    customer_features.columns = ["Member_number", "Recency", "Frequency", "Monetary", "Category_Diversity"]
        # ‚úÖ Fill NaN tr∆∞·ªõc khi scale
    customer_features = customer_features.fillna({
        "Recency": customer_features["Recency"].max(),  # gi·∫£ s·ª≠ kh√°ch ch∆∞a mua ‚Üí recency l·ªõn nh·∫•t
        "Frequency": 0,
        "Monetary": 0,
        "Category_Diversity": 0
    })
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X = scaler.fit_transform(customer_features.drop("Member_number", axis=1))

    # --- Clustering ---
    # KMeans
    kmeans_model = KMeans(n_clusters=4, random_state=42, n_init=10)
    customer_features['cluster_kmeans'] = kmeans_model.fit_predict(X)

    # GMM
    gmm_model = GaussianMixture(n_components=4, random_state=42)
    customer_features['cluster_gmm'] = gmm_model.fit_predict(X)

    # Agglomerative
    agg_model = AgglomerativeClustering(n_clusters=4)
    customer_features['cluster_agg'] = agg_model.fit_predict(X)

    # Hierarchical (Scipy)
    Z = linkage(X, method='ward')
    customer_features['cluster_hier'] = fcluster(Z, 4, criterion='maxclust')

    # Manual RFM segmentation
    #  --- Manual RFM segmentation ---
    try:
        r_labels = pd.qcut(customer_features["Recency"], 4, labels=[4,3,2,1], duplicates='drop')
        f_labels = pd.qcut(customer_features["Frequency"], 4, labels=[1,2,3,4], duplicates='drop')
        m_labels = pd.qcut(customer_features["Monetary"], 4, labels=[1,2,3,4], duplicates='drop')

        customer_features["R_quartile"] = r_labels.cat.codes.replace(-1, 0)  # -1 -> 0
        customer_features["F_quartile"] = f_labels.cat.codes.replace(-1, 0)
        customer_features["M_quartile"] = m_labels.cat.codes.replace(-1, 0)

        customer_features["RFM_Score"] = (
            customer_features["R_quartile"] + 
            customer_features["F_quartile"] + 
            customer_features["M_quartile"]
        )
        customer_features["cluster_rfm"] = pd.qcut(customer_features["RFM_Score"], 4, labels=[0,1,2,3], duplicates='drop')
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi t√≠nh RFM quartiles: {e}")
        customer_features["R_quartile"] = 0
        customer_features["F_quartile"] = 0
        customer_features["M_quartile"] = 0
        customer_features["RFM_Score"] = 0
        customer_features["cluster_rfm"] = 0
    # Spark ML
    # spark = SparkSession.builder.appName("CustomerSegmentation").getOrCreate()
    # spark_df = spark.createDataFrame(customer_features.drop(columns=[
    #     "cluster_kmeans","cluster_gmm","cluster_agg","cluster_hier","cluster_rfm",
    #     "R_quartile","F_quartile","M_quartile","RFM_Score"
    # ]))

    # assembler = VectorAssembler(inputCols=["Recency","Frequency","Monetary","Category_Diversity"], outputCol="features")
    # assembled = assembler.transform(spark_df).cache()

    # scaler_spark = SparkScaler(inputCol="features", outputCol="scaled_features", withStd=True, withMean=True)
    # scaler_model_spark = scaler_spark.fit(assembled)
    # scaled = scaler_model_spark.transform(assembled).cache()

    # spark_kmeans = SparkKMeans(k=4, seed=42, featuresCol="scaled_features", predictionCol="cluster_spark")
    # spark_kmeans_model = spark_kmeans.fit(scaled)
    # spark_clustered = spark_kmeans_model.transform(scaled)

    # df_spark_result = spark_clustered.select("Member_number", "cluster_spark").toPandas()
    # customer_features = customer_features.merge(df_spark_result, on="Member_number")

    return customer_features, X
# ===============================
# Compute RFM scores automatically
# ===============================
def compute_rfm_scores(df, user_id_col="Member_number", date_col="Date", prod_col="productId", amount_col=None):
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    snapshot_date = df[date_col].max() + pd.Timedelta(days=1)

    # Frequency = number of purchases
    freq = df.groupby(user_id_col)[prod_col].count()

    # Monetary
    if amount_col and amount_col in df.columns:
        monetary = df.groupby(user_id_col)[amount_col].sum()
    else:
        monetary = freq.copy()

    # Recency
    rec = df.groupby(user_id_col)[date_col].max().apply(lambda x: (snapshot_date - x).days)

    rfm = pd.DataFrame({"Recency": rec, "Frequency": freq, "Monetary": monetary})

    # ‚úÖ Fix NaN tr∆∞·ªõc khi ph√¢n lo·∫°i
    rfm = rfm.fillna({"Recency": rfm["Recency"].max(), "Frequency": 0, "Monetary": 0})

    # ‚úÖ D√πng cat.codes thay v√¨ astype(int)
    rfm["R"] = pd.qcut(rfm["Recency"], 4, labels=[4,3,2,1], duplicates="drop").cat.codes.replace(-1, 0)
    rfm["F"] = pd.qcut(rfm["Frequency"], 4, labels=[1,2,3,4], duplicates="drop").cat.codes.replace(-1, 0)
    rfm["M"] = pd.qcut(rfm["Monetary"], 4, labels=[1,2,3,4], duplicates="drop").cat.codes.replace(-1, 0)

    return rfm.reset_index()

try:
    rfm_scores = compute_rfm_scores(df_trans)
except Exception as e:
    st.warning(f"Could not compute RFM automatically: {e}")
    rfm_scores = None

# ===============================
# Classification into 6 segments
# ===============================
def classify_customer(R, F, M):
    if R >= 3 and F >= 3 and M == 4:
        return """VIP - Kh√°ch h√†ng trung th√†nh, gi√° tr·ªã cao \n
    -> Th∆∞·ªüng b·∫±ng ch∆∞∆°ng tr√¨nh VIP, ∆∞u ƒë√£i ƒë·ªôc quy·ªÅn \n
    & Cho quy·ªÅn truy c·∫≠p s·ªõm s·∫£n ph·∫©m/d·ªãch v·ª• m·ªõi \n
    & Khuy·∫øn kh√≠ch h·ªç tr·ªü th√†nh ƒë·∫°i s·ª© th∆∞∆°ng hi·ªáu"""
    elif F >= 2 and R >= 2 and M >= 2:
        return """Loyal Customers (Kh√°ch h√†ng trung th√†nh ti·ªÅm nƒÉng) \n
    -> TƒÉng c∆∞·ªùng g·∫Øn k·∫øt b·∫±ng ∆∞u ƒë√£i ƒë·ªãnh k·ª≥ \n
    & Cung c·∫•p g√≥i combo/bundles ph√π h·ª£p \n
    & ChƒÉm s√≥c c√° nh√¢n h√≥a ƒë·ªÉ ƒë·∫©y l√™n nh√≥m VIP"""
    elif R == 1 and F <= 1 and M <= 1:
        return """At Risk/ Lost (Kh√°ch h√†ng c√≥ nguy c∆° r·ªùi b·ªè) \n
    -> Gi·∫£m gi√° s·∫£n ph·∫©m y√™u th√≠ch tr∆∞·ªõc ƒë√¢y \n
    & T√¨m hi·ªÉu l√Ω do h·ªç √≠t quay l·∫°i (kh·∫£o s√°t) \n
    & Cung c·∫•p ∆∞u ƒë√£i ƒë·∫∑c bi·ªát ƒë·ªÉ khuy·∫øn kh√≠ch h·ªç quay l·∫°i \n
    & T·∫°o c·∫£m gi√°c c·∫•p b√°ch v·ªõi ∆∞u ƒë√£i gi·ªõi h·∫°n th·ªùi gian"""
    else:
        return """New/ Regular Customers (Kh√°ch h√†ng v√£ng lai) \n
    -> Chi·∫øn d·ªãch win-back v·ªõi khuy·∫øn m√£i l·ªõn \n
    & T·∫°o n·ªôi dung khuy·∫øn m√£i ƒë·ªÉ thu h√∫t kh√°ch h√†ng m·ªõi \n
    & Xin ƒë√°nh gi√° s·∫£n ph·∫©m/d·ªãch v·ª• ƒë·ªÉ c·∫£i thi·ªán """

# ===============================
# Classification from raw values (not quartiles)
# ===============================
def classify_customer_raw(recency, frequency, monetary,
                          r_thresh=60, f_thresh=10, m_thresh=1000):
    """
    Ph√¢n lo·∫°i kh√°ch h√†ng d·ª±a tr√™n gi√° tr·ªã R, F, M g·ªëc.
    - recency: s·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua g·∫ßn nh·∫•t
    - frequency: s·ªë l·∫ßn mua
    - monetary: t·ªïng chi ti√™u - (ƒë∆°n v·ªã: ngh√¨n ƒë·ªìng)
    Ng∆∞·ª°ng c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh theo d·ªØ li·ªáu th·ª±c t·∫ø.
    """
    if recency <= r_thresh and frequency >= f_thresh and monetary >= m_thresh:
        return """VIP - Kh√°ch h√†ng trung th√†nh, gi√° tr·ªã cao \n
    -> Th∆∞·ªüng b·∫±ng ch∆∞∆°ng tr√¨nh VIP, ∆∞u ƒë√£i ƒë·ªôc quy·ªÅn \n
    & Cho quy·ªÅn truy c·∫≠p s·ªõm s·∫£n ph·∫©m/d·ªãch v·ª• m·ªõi \n
    & Khuy·∫øn kh√≠ch h·ªç tr·ªü th√†nh ƒë·∫°i s·ª© th∆∞∆°ng hi·ªáu"""
    elif recency <= r_thresh and frequency >= f_thresh/2:
        return """Loyal Customers (Kh√°ch h√†ng trung th√†nh ti·ªÅm nƒÉng) \n
    -> TƒÉng c∆∞·ªùng g·∫Øn k·∫øt b·∫±ng ∆∞u ƒë√£i ƒë·ªãnh k·ª≥ \n
    & Cung c·∫•p g√≥i combo/bundles ph√π h·ª£p \n
    & ChƒÉm s√≥c c√° nh√¢n h√≥a ƒë·ªÉ ƒë·∫©y l√™n nh√≥m VIP"""
    elif recency > r_thresh*3 and frequency <= f_thresh/2 and monetary <= m_thresh/2:
        return """At Risk/ Lost (Kh√°ch h√†ng c√≥ nguy c∆° r·ªùi b·ªè) \n
    -> Gi·∫£m gi√° s·∫£n ph·∫©m y√™u th√≠ch tr∆∞·ªõc ƒë√¢y \n
    & T√¨m hi·ªÉu l√Ω do h·ªç √≠t quay l·∫°i (kh·∫£o s√°t) \n
    & Cung c·∫•p ∆∞u ƒë√£i ƒë·∫∑c bi·ªát ƒë·ªÉ khuy·∫øn kh√≠ch h·ªç quay l·∫°i \n
    & T·∫°o c·∫£m gi√°c c·∫•p b√°ch v·ªõi ∆∞u ƒë√£i gi·ªõi h·∫°n th·ªùi gian"""
    else:
        return """New/ Regular Customers (Kh√°ch h√†ng v√£ng lai) \n
    -> Chi·∫øn d·ªãch win-back v·ªõi khuy·∫øn m√£i l·ªõn \n
    & T·∫°o n·ªôi dung khuy·∫øn m√£i ƒë·ªÉ thu h√∫t kh√°ch h√†ng m·ªõi \n
    & Xin ƒë√°nh gi√° s·∫£n ph·∫©m/d·ªãch v·ª• ƒë·ªÉ c·∫£i thi·ªán """

# Optimized clustering evaluation and comparison

def evaluate_clustering_performance(scaled_features, customer_data):
    """
    Evaluate clustering methods with Silhouette, ARI, NMI
    """
    clustering_methods = {
        "KMeans": "cluster_kmeans",
        "GMM": "cluster_gmm",
        "Agglomerative": "cluster_agg", 
        "Hierarchical": "cluster_hier",
        "SparkKMeans": "cluster_spark"
    }
    
    # --- Silhouette Scores ---
    silhouette_scores = {}
    for method_name, cluster_col in clustering_methods.items():
        if cluster_col in customer_data.columns:
            score = silhouette_score(scaled_features, customer_data[cluster_col])
            silhouette_scores[method_name] = round(score, 4)
    
    # --- Pairwise ARI & NMI ---
    similarity_results = []
    method_columns = list(clustering_methods.values())

    for i in range(len(method_columns)):
        for j in range(i+1, len(method_columns)):
            col1, col2 = method_columns[i], method_columns[j]
            if col1 in customer_data.columns and col2 in customer_data.columns:
                ari = adjusted_rand_score(customer_data[col1], customer_data[col2])
                nmi = normalized_mutual_info_score(customer_data[col1], customer_data[col2])
                similarity_results.append([
                    col1.replace('cluster_', '').title(),
                    col2.replace('cluster_', '').title(),
                    round(ari, 4),
                    round(nmi, 4)
                ])
    
    similarity_df = pd.DataFrame(similarity_results, columns=["Method A", "Method B", "ARI", "NMI"])
    return silhouette_scores, similarity_df


# ===============================
# Sidebar Menu
# ===============================
menu = st.sidebar.radio(
    "Menu",
    [
        "Introduction",
        "Business Problem",
        "Evaluation & Report & Comparison",
        "New Prediction / Analysis"
    ]
)
st.sidebar.header("Data")
product_up = st.sidebar.file_uploader("Upload Products_with_categories.csv", type=["csv"])
transaction_up = st.sidebar.file_uploader("Upload Transactions.csv", type=["csv"])
if product_up is None:
    df_products = pd.read_csv("Products_with_Categories.csv")
if transaction_up is None:
    df_trans = pd.read_csv("Transactions.csv")
st.sidebar.markdown("---")
# ===============================
# Sidebar Threshold Settings (for raw RFM)
# ===============================
st.sidebar.markdown("### ‚öôÔ∏è Thi·∫øt l·∫≠p ng∆∞·ª°ng RFM (Raw Values)")
r_thresh = st.sidebar.number_input("Ng∆∞·ª°ng Recency (ng√†y)", min_value=1, value=90)
f_thresh = st.sidebar.number_input("Ng∆∞·ª°ng Frequency (s·ªë l·∫ßn mua)", min_value=1, value=10)
m_thresh = st.sidebar.number_input("Ng∆∞·ª°ng Monetary (t·ªïng chi ti√™u - ƒëvt: ngh√¨n ƒë·ªìng)", min_value=1, value=500)
st.sidebar.markdown("---")

# ===============================
# Business Problem
# ===============================
if menu == "Business Problem":
    st.title("üìå Business Problem")
    st.write("""
    C·ª≠a h√†ng X ch·ªß y·∫øu b√°n c√°c s·∫£n ph·∫©m thi·∫øt y·∫øu cho kh√°ch h√†ng nh∆∞:
    rau, c·ªß, qu·∫£, th·ªãt, c√°, tr·ª©ng, s·ªØa, n∆∞·ªõc gi·∫£i kh√°t...\n
    Kh√°ch h√†ng c·ªßa c·ª≠a h√†ng th∆∞·ªùng l√† kh√°ch h√†ng mua l·∫ª.
    """)
    st.write("""-> Ch·ªß c·ª≠a h√†ng X mong mu·ªën c√≥ th·ªÉ b√°n ƒë∆∞·ª£c nhi·ªÅu h√†ng h√≥a h∆°n
    c≈©ng nh∆∞ gi·ªõi thi·ªáu s·∫£n ph·∫©m ƒë·∫øn ƒë√∫ng ƒë·ªëi t∆∞·ª£ng kh√°ch h√†ng,
    t·ª´ ƒë√≥ tƒÉng doanh thu v√† l·ª£i nhu·∫≠n.
    ƒê·ªìng th·ªùi, vi·ªác ph√¢n kh√∫c kh√°ch h√†ng c≈©ng gi√∫p c·ª≠a h√†ng c√≥ chi·∫øn l∆∞·ª£c
    chƒÉm s√≥c v√† l√†m h√†i l√≤ng kh√°ch h√†ng.
    """)
    st.image('customersegment.png', caption='Customer Segmentation', width=800)

# ===============================
# Evaluation & Report
# ===============================
elif menu == "Evaluation & Report & Comparison":
    st.title("üìä Evaluation & Report & Comparison")
    st.image('segment1.png', caption='Customer Segment Pros', width=600)

    tab = st.tabs(["RFM Analysis, Segmentation & Visualization",
                "Clustering Model Comparison & Evaluation"])
    # ---------------- Tab 1 ----------------
    with tab[0]:
        # ∆Øu ti√™n d·ªØ li·ªáu upload
        if product_up is not None:
            df_products = pd.read_csv(product_up)
            st.success("‚úÖ Products data uploaded successfully.")
            st.dataframe(df_products.head())
        else:
            df_products = df_products
            st.info("‚ö†Ô∏è D√πng d·ªØ li·ªáu Products m·∫∑c ƒë·ªãnh.")

        if transaction_up is not None:
            df_trans = pd.read_csv(transaction_up)
            st.success("‚úÖ Transactions data uploaded successfully.")
            st.dataframe(df_trans.head())
        else:
            df_trans = df_trans
            st.info("‚ö†Ô∏è D√πng d·ªØ li·ªáu Transactions m·∫∑c ƒë·ªãnh.")

        # T√≠nh to√°n RFM
        if rfm_scores is not None:
            rfm_scores = compute_rfm_scores(df_trans)
            rfm_scores["Segment_Raw"] = rfm_scores.apply(
                lambda x: classify_customer_raw(x["Recency"], x["Frequency"], x["Monetary"],
                                                r_thresh, f_thresh, m_thresh), axis=1
            )
            st.success("‚úÖ RFM scores computed successfully.")
        # except Exception as e:
        #     st.error(f"‚ùå L·ªói khi t√≠nh RFM: {e}")
        #     rfm_scores = None

        # N·∫øu c√≥ RFM th√¨ v·∫Ω bi·ªÉu ƒë·ªì
        if rfm_scores is not None:
            st.subheader("üìà RFM Score Distribution")
            fig, axes = plt.subplots(1, 3, figsize=(15, 4))
            sns.countplot(x="R", data=rfm_scores, ax=axes[0])
            axes[0].set_title("Recency Score")
            sns.countplot(x="F", data=rfm_scores, ax=axes[1])
            axes[1].set_title("Frequency Score")
            sns.countplot(x="M", data=rfm_scores, ax=axes[2])
            axes[2].set_title("Monetary Score")
            st.pyplot(fig)

            # Ph√¢n lo·∫°i Segment
            rfm_scores["Segment"] = rfm_scores.apply(
                lambda x: classify_customer(x["R"], x["F"], x["M"]), axis=1
            )
            seg_counts = rfm_scores["Segment"].value_counts()

            st.bar_chart(seg_counts)
            st.dataframe(rfm_scores.head())
        else:
            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu RFM ƒë·ªÉ hi·ªÉn th·ªã.")

    # ---------------- Tab 2 ----------------
    with tab[1]:
        st.subheader("Clustering Model Comparison")

    try:
        customer_data, scaled_features = customer_features(df_products, df_trans)
        silhouette_scores, similarity_df = evaluate_clustering_performance(scaled_features, customer_data)

        # Hi·ªÉn th·ªã Silhouette
        st.subheader("üìä Silhouette Scores")
        df_metrics = pd.DataFrame(list(silhouette_scores.items()), columns=["Model", "Silhouette"])
        st.table(df_metrics)

        # Hi·ªÉn th·ªã ARI & NMI
        st.subheader("üîó Similarity Analysis (ARI & NMI)")
        st.dataframe(similarity_df)

        # Heatmap
        fig, ax = plt.subplots(figsize=(6,4))
        pivot_table = similarity_df.pivot(index="Method A", columns="Method B", values="ARI")
        sns.heatmap(pivot_table, annot=True, cmap="Blues", ax=ax)
        ax.set_title("Heatmap of ARI")
        st.pyplot(fig)
        plt.close(fig)

    except Exception as e:
        st.error(f"‚ùå L·ªói khi t√≠nh to√°n clustering: {e}")
        st.write("""üîπ K·∫øt qu·∫£ so s√°nh nhanh
- KMeans & SparkKMeans:
Silhouette t·ªët nh·∫•t (‚âà0.285).
ARI & NMI g·∫ßn¬†1.0¬†‚Üí \n 
\t Hai ph∆∞∆°ng ph√°p n√†y g·∫ßn nh∆∞ gi·ªëng h·ªát nhau.
\n \t SparkKMeans c√≥ l·ª£i th·∫ø v·ªÅ¬†kh·∫£ nƒÉng m·ªü r·ªông¬†(ph√π h·ª£p khi d·ªØ li·ªáu l·ªõn). \n
- Agglomerative & Hierarchical:
K·∫øt qu·∫£ gi·ªëng h·ªát nhau (ARI = 1.0, NMI = 1.0).
\n \t Tuy nhi√™n, Silhouette th·∫•p h∆°n (‚âà0.216). \n
\t ∆Øu ƒëi·ªÉm:¬†tr·ª±c quan ho√° b·∫±ng dendrogram, d·ªÖ gi·∫£i th√≠ch m·ªëi quan h·ªá ph√¢n c·∫•p.
- GMM (Gaussian Mixture):
Silhouette th·∫•p nh·∫•t (‚âà0.159).
\n \t ARI & NMI v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c ƒë·ªÅu th·∫•p ‚Üí t·∫°o ra ph√¢n c·ª•m r·∫•t kh√°c bi·ªát.
\n \t Ch·ªâ ph√π h·ª£p n·∫øu mu·ªën¬†ph√¢n c·ª•m m·ªÅm (soft clustering), t·ª©c m·ªôt kh√°ch h√†ng c√≥ th·ªÉ thu·ªôc nhi·ªÅu c·ª•m.
\n -> üîπ Khuy·∫øn ngh·ªã cu·ªëi c√πng
\n - Ch·ªçn KMeans (ho·∫∑c SparkKMeans n·∫øu d·ªØ li·ªáu l·ªõn)¬†l√†m ph∆∞∆°ng ph√°p ch√≠nh ƒë·ªÉ ph√¢n c·ª•m RFM.
\n \t L√Ω do: ƒëi·ªÉm Silhouette t·ªët nh·∫•t, t√≠nh ·ªïn ƒë·ªãnh cao, d·ªÖ √°p d·ª•ng cho business.
SparkKMeans ƒë·∫∑c bi·ªát ph√π h·ª£p n·∫øu d·ªØ li·ªáu ng√†y c√†ng m·ªü r·ªông (big data).
\n - D√πng Agglomerative/Hierarchical¬†nh∆∞ m·ªôt¬†ph∆∞∆°ng ph√°p b·ªï tr·ª£¬†ƒë·ªÉ ki·ªÉm tra l·∫°i k·∫øt qu·∫£ v√† tr·ª±c quan h√≥a m·ªëi quan h·ªá ph√¢n c·ª•m.
\n - GMM¬†c√≥ th·ªÉ th·ª≠ nghi·ªám n·∫øu mu·ªën ki·ªÉm tra xem kh√°ch h√†ng c√≥ th·ªÉ thu·ªôc¬†nhi·ªÅu nh√≥m ƒë·ªìng th·ªùi¬†(ph√¢n t√≠ch n√¢ng cao).
\n - C√≥ th·ªÉ d√πng RFM Manual cho kh·∫£ nƒÉng t∆∞∆°ng t√°c c·ªßa end-user d·ªÖ h∆°n
""")

# ===============================
# New Prediction (manual input)
# ===============================
elif menu == "New Prediction / Analysis":
    st.title("üîÆ Customer Prediction (Manual Input)")
    st.write("""- **VIP (High Frequency, High Monetary, Low Recency)** 
\n -> Shop very often, spend a lot, and bought recently.
\n - **Loyal Customer (Medium Frequency & Monetary, Recent Buyers)** 
\n -> Bought recently, moderate spending, could become loyal. They‚Äôre¬†on the path to becoming VIP.
\n - **At Risk/ Lost Customer (Low Frequency, Low Monetary, High Recency)** 
\n -> Send reminders, discounts to reactivate.  
\n - **Regular/ New Customer (High Recency, Low Frequency & Monetary)** 
\n -> Just purchased or bought only once. Still deciding whether to stick with you.""")
    option = st.radio("Ch·ªçn ki·ªÉu nh·∫≠p d·ªØ li·ªáu:", ["Nh·∫≠p ƒëi·ªÉm RFM (1‚Äì4)", "Nh·∫≠p gi√° tr·ªã g·ªëc R, F, M"])
    if option == "Nh·∫≠p ƒëi·ªÉm RFM (1‚Äì4)":
    # ---------------------------
    # Option 1: nh·∫≠p ƒëi·ªÉm RFM
    # ---------------------------
            st.subheader("üîÆ D·ª± ƒëo√°n theo ƒëi·ªÉm RFM (1‚Äì4) - Single prediction")
            col1, col2, col3 = st.columns(3)
            with col1:
                R = st.slider("Recency Score (1‚Äì4)", 1, 4, 2)
            with col2:
                F = st.slider("Frequency Score (1‚Äì4)", 1, 4, 2)
            with col3:
                M = st.slider("Monetary Score (1‚Äì4)", 1, 4, 2)

            if st.button("Predict by Score"):
                segment = classify_customer(R, F, M)
                st.success(f"üè∑Ô∏è This customer belongs to: **{segment}**")

            st.markdown("---")
            st.subheader("üìÇ Bulk Prediction (File RFM Score)")
            file = st.file_uploader("T·∫£i l√™n file CSV/Excel v·ªõi c·ªôt `R`, `F`, `M`", type=["csv","xlsx"])
            if file:
                if file.name.endswith(".csv"):
                    df_input = pd.read_csv(file)
                else:
                    df_input = pd.read_excel(file)

                if {"R","F","M"}.issubset(df_input.columns):
                    df_input["Segment"] = df_input.apply(lambda x: classify_customer(x["R"], x["F"], x["M"]), axis=1)
                    st.dataframe(df_input.head(20))
                    st.download_button(
                        "T·∫£i xu·ªëng d·ª± ƒëo√°n (RFM Score)",
                        data=df_input.to_csv(index=False).encode("utf-8"),
                        file_name="rfm_predictions_score.csv",
                        mime="text/csv"
                    )
                else:
                    st.error("File ph·∫£i ch·ª©a c√°c c·ªôt `R`, `F`, `M`.")

    # ---------------------------
    # Option 2: nh·∫≠p gi√° tr·ªã g·ªëc
    # ---------------------------
    else: 
        st.markdown("---")
        st.subheader("üîÆ D·ª± ƒëo√°n theo gi√° tr·ªã g·ªëc")

        col1, col2, col3 = st.columns(3)
        with col1:
            recency_val = st.number_input("Recency (ng√†y k·ªÉ t·ª´ l·∫ßn mua g·∫ßn nh·∫•t)", min_value=0, value=100)
        with col2:
            frequency_val = st.number_input("Frequency (s·ªë l·∫ßn mua)", min_value=0, value=5)
        with col3:
            monetary_val = st.number_input("Monetary (t·ªïng chi ti√™u - ƒë∆°n v·ªã: ngh√¨n ƒë·ªìng)", min_value=0, value=200)

        if st.button("Predict by Raw Value"):
            segment = classify_customer_raw(recency_val, frequency_val, monetary_val,
                                                    r_thresh=r_thresh, f_thresh=f_thresh, m_thresh=m_thresh)
            st.success(f"üè∑Ô∏è This customer belongs to: **{segment}**")

        st.markdown("---")
        st.subheader("üìÇ Bulk Prediction (File Raw Values)")
        file = st.file_uploader("T·∫£i l√™n file CSV/Excel v·ªõi c·ªôt `R - Recency`, `F - Frequency`, `M - Monetary`", type=["csv","xlsx"])
        if file:
            if file.name.endswith(".csv"):
                df_input = pd.read_csv(file)
            else:
                df_input = pd.read_excel(file)

            if {"Recency","Frequency","Monetary"}.issubset(df_input.columns):
                df_input["Segment"] = df_input.apply(
                    lambda x: classify_customer_raw(
                        x["Recency"], x["Frequency"], x["Monetary"],
                        r_thresh=r_thresh, f_thresh=f_thresh, m_thresh=m_thresh
                        ),
                    axis=1
                    )
                st.dataframe(df_input.head(20))
                st.download_button(
                    "T·∫£i xu·ªëng d·ª± ƒëo√°n (Raw Values)",
                    data=df_input.to_csv(index=False).encode("utf-8"),
                    file_name="rfm_predictions_raw.csv",
                    mime="text/csv"
                )
            else:
                st.error("File ph·∫£i ch·ª©a c√°c c·ªôt `R`, `F`, `M`.")

        # st.markdown("---")
        # st.title("üìÇ Bulk Prediction (Upload file)")
        # option = st.radio("Ch·ªçn ki·ªÉu nh·∫≠p d·ªØ li·ªáu:", ["Nh·∫≠p ƒëi·ªÉm RFM (1‚Äì4)", "Nh·∫≠p gi√° tr·ªã g·ªëc R, F, M"])
        # if option == "ƒêi·ªÉm RFM (1‚Äì4)":
        #     st.write("T·∫£i l√™n m·ªôt t·ªáp CSV/Excel v·ªõi c√°c c·ªôt `R - Recency`, `F - Frequency`, `M - Monetary` ƒë·ªÉ ph√¢n lo·∫°i kh√°ch h√†ng.")

        #     file = st.file_uploader("T·∫£i l√™n CSV ho·∫∑c Excel", type=["csv","xlsx"])
        #     if file:
        #         if file.name.endswith(".csv"):
        #             df_input = pd.read_csv(file)
        #         else:
        #             df_input = pd.read_excel(file)

        #         if {"R","F","M"}.issubset(df_input.columns):
        #             df_input["Segment"] = df_input.apply(lambda x: classify_customer(x["R"], x["F"], x["M"]), axis=1)
        #             st.dataframe(df_input.head(20))
        #             st.download_button(
        #                 "T·∫£i xu·ªëng d·ª± ƒëo√°n",
        #                 data=df_input.to_csv(index=False).encode("utf-8"),
        #                 file_name="rfm_predictions.csv",
        #                 mime="text/csv"
                    # )
        #         else:
        #             st.error("File ph·∫£i ch·ª©a c√°c c·ªôt `R`, `F`, `M`.")
        # else:
        #     st.write("üìÇ Gi√° tr·ªã RFM g·ªëc")
        #     file = st.file_uploader("T·∫£i l√™n file CSV/Excel v·ªõi c·ªôt `R - Recency`, `F - Frequency`, `M - Monetary`", type=["csv","xlsx"])
        #     if file:
        #         if file.name.endswith(".csv"):
        #             df_input = pd.read_csv(file)
        #         else:
        #             df_input = pd.read_excel(file)

        #         if {"Recency","Frequency","Monetary"}.issubset(df_input.columns):
        #             df_input["Segment"] = df_input.apply(
        #                 lambda x: classify_customer_raw(x["Recency"], x["Frequency"], x["Monetary"]),
        #                 axis=1
        #             )
        #             st.dataframe(df_input.head(20))
        #             st.download_button(
        #                 "T·∫£i xu·ªëng d·ª± ƒëo√°n (Raw Values)",
        #                 data=df_input.to_csv(index=False).encode("utf-8"),
        #                 file_name="rfm_predictions_raw.csv",
        #                 mime="text/csv"
        #             )
        #         else:
        #             st.error("File ph·∫£i ch·ª©a c√°c c·ªôt `R`, `F`, `M`.")
# ===============================
# Introduction
# ===============================
elif menu == "Introduction":
    st.title("üë®‚Äçüíª Introduction")
    st.write("""
    - **T√™n**: Tr·∫ßn Nh·∫≠t Minh   
    - **Email**: nhatminhtr233@gmail.com   
    - **GVHD**: Khu·∫•t Thu·ª≥ Ph∆∞∆°ng
    - **Project**: Customer Segmentation
    """)
    st.image("RFM_clustering.png", caption="RFM Clustering")
    st.subheader("üîÑ Project Pipeline")
    st.write = [
        ("Business Problem", "X√°c ƒë·ªãnh m·ª•c ti√™u kinh doanh, v√≠ d·ª•: tƒÉng doanh thu, chƒÉm s√≥c kh√°ch h√†ng."),
        ("Data Preparation", "Thu th·∫≠p & l√†m s·∫°ch d·ªØ li·ªáu s·∫£n ph·∫©m v√† giao d·ªãch."),
        ("RFM Analysis", "T√≠nh to√°n Recency, Frequency, Monetary cho t·ª´ng kh√°ch h√†ng."),
        ("Clustering Models", "Th·ª≠ nhi·ªÅu m√¥ h√¨nh: KMeans, GMM, Agglomerative, Hierarchical, SparkKMeans."),
        ("Evaluation", "ƒê√°nh gi√° b·∫±ng Silhouette, ARI, NMI ƒë·ªÉ so s√°nh m√¥ h√¨nh."),
        ("Recommendation & Deployment", "ƒê∆∞a ra g·ª£i √Ω kinh doanh cho t·ª´ng nh√≥m kh√°ch h√†ng v√† tri·ªÉn khai h·ªá th·ªëng.")
    ]
    st.image('pipeline.png', caption = 'Project Pipeline')
